{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dce17d52-9355-49e0-88be-276ab5f013ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from io import StringIO\n",
    "from datetime import datetime as t\n",
    "import time,boto3,os,psycopg2,pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "access_key_id  = \"xxxxxxxxxxxxxxx\" \n",
    "access_key_pwd = \"xxxxxxxxxxxxxxx\"\n",
    "postgres_pwd   = 'xxxxxxxxxxxxxxx' \n",
    "\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = access_key_id\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = access_key_pwd\n",
    "\n",
    "boto3.client('s3', aws_access_key_id=access_key_id, aws_secret_access_key=access_key_pwd)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key_id)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", access_key_pwd)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "\n",
    "\n",
    "jar_path = \"dbfs:/FileStore/postgresql_42_7_6.jar\"\n",
    "sc._jsc.addJar(jar_path)\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://database-2.czy4sq8iitbm.ap-south-1.rds.amazonaws.com:5432/postgres\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": postgres_pwd,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed66c5b4-9c11-4ab7-ba71-01998f783504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_postgres_data(df):\n",
    "\n",
    "    conn,cur = postgres_cur()\n",
    "\n",
    "    grouped_df = df.groupBy(col('customer'), col('merchant')).agg(F.count(\"*\").alias(\"txn_count\"),F.avg(\"amount\").alias(\"avg_amount\"))\n",
    "    grouped_df.write.jdbc(url=jdbc_url,table=\"staging_customer_merchant_txn_counts\", mode=\"overwrite\",properties=properties)\n",
    "    upsert_sql = \"\"\"\n",
    "                    INSERT INTO customer_merchant_txn_counts (customer, merchant, txn_count, avg_amount)\n",
    "                    SELECT customer, merchant, txn_count, avg_amount\n",
    "                    FROM staging_customer_merchant_txn_counts\n",
    "                    ON CONFLICT (customer, merchant)\n",
    "                    DO UPDATE \n",
    "                    SET txn_count = customer_merchant_txn_counts.txn_count + EXCLUDED.txn_count,\n",
    "                        avg_amount = (\n",
    "                        (customer_merchant_txn_counts.avg_amount * customer_merchant_txn_counts.txn_count + EXCLUDED.avg_amount * EXCLUDED.txn_count) /\n",
    "                        (customer_merchant_txn_counts.txn_count + EXCLUDED.txn_count)\n",
    "                        );\n",
    "                \"\"\"\n",
    "    cur.execute(upsert_sql)\n",
    "    conn.commit()\n",
    "\n",
    "    cur.execute(\"DROP TABLE IF EXISTS staging_customer_merchant_txn_counts\")\n",
    "    conn.commit()\n",
    "\n",
    "    grouped_gender = df.groupBy(\"merchant\").agg(F.sum(F.when(col(\"gender\") == \"'M'\", 1).otherwise(0)).alias(\"male\"),F.sum(F.when(col(\"gender\") == \"'F'\", 1).otherwise(0)).alias(\"female\"))\n",
    "    grouped_gender.write.jdbc(url=jdbc_url,table=\"staging_merchant_gender_counts\", mode=\"overwrite\",properties=properties)\n",
    "\n",
    "    upsert_sql1 =\"\"\"\n",
    "                    INSERT INTO merchant_gender_counts (merchant,male,female)\n",
    "                    SELECT merchant, male, female\n",
    "                    FROM staging_merchant_gender_counts\n",
    "                    ON CONFLICT (merchant)\n",
    "                    DO UPDATE \n",
    "                    SET male   = merchant_gender_counts.male + EXCLUDED.male,\n",
    "                        female = merchant_gender_counts.female + EXCLUDED.female;\n",
    "                \"\"\"\n",
    "        \n",
    "    cur.execute(upsert_sql1)\n",
    "    conn.commit()\n",
    "    \n",
    "    cur.execute(\"DROP TABLE IF EXISTS staging_merchant_gender_counts\")\n",
    "    conn.commit()\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def pat1(jdbc_url,properties):\n",
    "\n",
    "\n",
    "    customer_merchant_txn_counts = spark.read.jdbc(url=jdbc_url, table=\"customer_merchant_txn_counts\", properties=properties)\n",
    "\n",
    "    window_spec_by_merchant = Window.partitionBy(\"merchant\").orderBy(F.desc(\"txn_count\"))\n",
    "\n",
    "    ranked_df = customer_merchant_txn_counts.withColumn(\"txn_rank\", F.rank().over(window_spec_by_merchant))\n",
    "    ranked_df = ranked_df.withColumn(\"total_txn\", F.sum(\"txn_count\").over(window_spec_by_merchant))\n",
    "\n",
    "    top_1pct_customers_df = ranked_df.filter((col(\"txn_rank\") <= F.greatest(F.lit(1), (col(\"total_txn\") * 0.01).cast(\"int\"))))\n",
    "    top_1pct_customers_df = top_1pct_customers_df.select(\"customer\", \"merchant\", \"txn_count\", \"txn_rank\", \"total_txn\")\n",
    "\n",
    "    bottom_1pct_weights_df = spark.read.jdbc(url=jdbc_url, table=\"bottom_1pct_weights\", properties=properties)\n",
    "    joined_df = top_1pct_customers_df.alias(\"t\").join(bottom_1pct_weights_df.alias(\"b\"),on=[\"customer\", \"merchant\"],how=\"inner\")\n",
    "\n",
    "    # Add the additional columns\n",
    "    pattern_matches_df = joined_df.withColumn(\"ystarttime\", F.current_timestamp())\n",
    "    pattern_matches_df = pattern_matches_df.withColumn(\"detectiontime\", F.current_timestamp())\n",
    "    pattern_matches_df = pattern_matches_df.withColumn(\"patternid\", F.lit(\"PatId1\"))\n",
    "    pattern_matches_df = pattern_matches_df.withColumn(\"actiontype\",F.lit(\"UPGRADE\"))\n",
    "\n",
    "\n",
    "    # Select required columns in order\n",
    "    pattern_matches_df = pattern_matches_df.select(\n",
    "        \"ystarttime\",\n",
    "        \"detectiontime\",\n",
    "        \"patternid\",\n",
    "        \"actiontype\",\n",
    "        \"customer\",\n",
    "        \"merchant\"\n",
    "    )\n",
    "\n",
    "    return pattern_matches_df\n",
    "\n",
    "def pat2(jdbc_url,properties):\n",
    "\n",
    "    customer_merchant_txn_counts = spark.read.jdbc(url=jdbc_url, table=\"customer_merchant_txn_counts\", properties=properties) \n",
    "    \n",
    "    child_customers_df = customer_merchant_txn_counts.filter((col(\"avg_amount\") < 23) & (col(\"txn_count\") >= 80))\n",
    "    child_customers_df = child_customers_df.withColumn(\"ystarttime\", F.current_timestamp())\n",
    "    child_customers_df = child_customers_df.withColumn(\"detectiontime\", F.current_timestamp())\n",
    "    child_customers_df = child_customers_df.withColumn(\"patternid\", F.lit(\"PatId2\"))\n",
    "    child_customers_df = child_customers_df.withColumn(\"actiontype\",F.lit(\"CHILD\"))\n",
    "    child_customers_df = child_customers_df.select(\n",
    "        \"ystarttime\",\n",
    "        \"detectiontime\",\n",
    "        \"patternid\",\n",
    "        \"actiontype\",\n",
    "        \"customer\",\n",
    "        \"merchant\"\n",
    "    )\n",
    "    return child_customers_df\n",
    "\n",
    "def pat3(jdbc_url,properties):\n",
    "\n",
    "    merchant_gender = spark.read.jdbc(url=jdbc_url, table=\"merchant_gender_counts\", properties=properties)\n",
    "    merchant_gender = merchant_gender.filter((col('male') > col('female')) & (col('female') > 100))\n",
    "\n",
    "    merchant_gender = merchant_gender.withColumn(\"ystarttime\", F.current_timestamp())\n",
    "    merchant_gender = merchant_gender.withColumn(\"detectiontime\", F.current_timestamp())\n",
    "    merchant_gender = merchant_gender.withColumn(\"patternid\", F.lit(\"PatId3\"))\n",
    "    merchant_gender = merchant_gender.withColumn(\"actiontype\",F.lit(\"DEI-NEEDED\"))\n",
    "\n",
    "    merchant_gender = merchant_gender.select(\n",
    "        \"ystarttime\",\n",
    "        \"detectiontime\",\n",
    "        \"patternid\",\n",
    "        \"actiontype\",\n",
    "        F.lit(\"\").alias(\"customer\"),\n",
    "        \"merchant\"\n",
    "    )\n",
    "    return merchant_gender\n",
    "\n",
    "def postgres_cur():\n",
    "    host = \"database-2.czy4sq8iitbm.ap-south-1.rds.amazonaws.com\"\n",
    "    port = \"5432\"\n",
    "    db   = \"postgres\"\n",
    "    user_name = \"postgres\"\n",
    "    user_pass = postgres_pwd\n",
    "    conn = psycopg2.connect(host=host,port=port,database=db,user=user_name,password=user_pass)\n",
    "    return conn,conn.cursor()\n",
    "\n",
    "def get_next_chunk():\n",
    "    conn,cur = postgres_cur()\n",
    "    cur.execute(\"\"\"\n",
    "                            SELECT id, chunk_index, s3_path \n",
    "                            FROM chunk_tracking \n",
    "                            WHERE status = 'written' \n",
    "                            ORDER BY chunk_index ASC \n",
    "                            LIMIT 1\n",
    "                            FOR UPDATE SKIP LOCKED;\n",
    "                    \"\"\")\n",
    "    data = cur.fetchone()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return data\n",
    "\n",
    "def dedupe_final_output():\n",
    "    \n",
    "    conn,cur = postgres_cur()\n",
    "    cur.execute(\n",
    "                \"\"\"\n",
    "                    WITH ranked_rows AS (\n",
    "                        SELECT ctid,\n",
    "                            ROW_NUMBER() OVER (\n",
    "                                PARTITION BY customer, merchant\n",
    "                                ORDER BY ystarttime ASC\n",
    "                            ) AS rn\n",
    "                        FROM final_output\n",
    "                    )\n",
    "                    DELETE FROM final_output\n",
    "                    WHERE ctid IN (\n",
    "                        SELECT ctid FROM ranked_rows WHERE rn > 1);\n",
    "                \"\"\")\n",
    "    conn.commit() \n",
    "    # cur.close()\n",
    "    # conn.close()\n",
    "    return True\n",
    "\n",
    "\n",
    "def update_chunk_status(chunk_index, status):\n",
    "    conn,cur = postgres_cur()\n",
    "\n",
    "    cur.execute(\"BEGIN;\")\n",
    "\n",
    "    # Lock the row first\n",
    "    cur.execute(\"\"\"\n",
    "                    SELECT * FROM chunk_tracking\n",
    "                    WHERE chunk_index = %s\n",
    "                    FOR UPDATE;\n",
    "                \"\"\", (chunk_index,))\n",
    "\n",
    "    # Now do the update safely\n",
    "    cur.execute(\"\"\"\n",
    "                    UPDATE chunk_tracking \n",
    "                    SET status = %s, updated_at = %s \n",
    "                    WHERE chunk_index = %s;\n",
    "                \"\"\", (status, datetime.utcnow(), chunk_index))\n",
    "\n",
    "    cur.execute(\"COMMIT;\")\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def list_s3_files(bucket_name, prefix):\n",
    "\n",
    "    s3_client = boto3.client('s3')\n",
    "    file_list = []\n",
    "    \n",
    "    \n",
    "    if prefix and not prefix.endswith('/'):\n",
    "        prefix += '/'\n",
    "\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    try:\n",
    "        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "            if 'Contents' in page:\n",
    "                for obj in page['Contents']:\n",
    "                    file_list.append(obj['Key'])\n",
    "\n",
    "        file_list = [file for file in file_list if file.lower().endswith('.csv')]\n",
    "        return file_list[0]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "    \n",
    "def generate_output():\n",
    "\n",
    "    conn,cur = postgres_cur()\n",
    "    query = \"\"\"         \n",
    "                    WITH to_publish AS (\n",
    "                        SELECT merchant, customer\n",
    "                        FROM final_output\n",
    "                        WHERE published IS NULL\n",
    "                        ORDER BY ystarttime\n",
    "                        LIMIT 50\n",
    "                        FOR UPDATE SKIP LOCKED),\n",
    "                check_count AS (\n",
    "                    SELECT COUNT(*) AS cnt FROM to_publish\n",
    "                ),\n",
    "                update_rows AS (\n",
    "                    UPDATE final_output\n",
    "                    SET published = 'published'\n",
    "                    WHERE (merchant, customer) IN (\n",
    "                        SELECT merchant, customer FROM to_publish\n",
    "                    )\n",
    "                    AND EXISTS (\n",
    "                        SELECT 1 FROM check_count WHERE cnt = 50\n",
    "                    )\n",
    "                    RETURNING *\n",
    "                )\n",
    "                SELECT * FROM update_rows;\n",
    "            \"\"\"\n",
    "    cur.execute(\"BEGIN;\")\n",
    "    cur.execute(query)\n",
    "    published_rows = cur.fetchall()\n",
    "    cur.execute(\"COMMIT;\")\n",
    "\n",
    "    schema = StructType([\n",
    "    StructField(\"ystarttime\", TimestampType(), True),\n",
    "    StructField(\"detectiontime\",TimestampType(), True),\n",
    "    StructField(\"patternid\",StringType(), True),\n",
    "    StructField(\"actiontype\",StringType(), True),\n",
    "    StructField(\"customer\",StringType(), True),\n",
    "    StructField(\"merchant\",StringType(), True),\n",
    "    StructField(\"published\",StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # column_names = [desc[0] for desc in cur.description]\n",
    "    df = spark.createDataFrame(published_rows,schema=schema)\n",
    "    df = df.select(col(\"ystarttime\"),col(\"detectiontime\"),col(\"patternid\"),col(\"actiontype\"),col(\"customer\"),col(\"merchant\"))\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bfcda40-bb9c-4a9b-86e7-93eac2006701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Functions and theirs descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "810280db-3d97-49ff-b25c-8c99609b4d2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to update aggregated transaction data into PostgreSQL\n",
    "def update_postgres_data(df):\n",
    "    conn, cur = postgres_cur()\n",
    "\n",
    "    # Group by customer and merchant, aggregate transaction count and average amount\n",
    "    grouped_df = df.groupBy(col('customer'), col('merchant')).agg(\n",
    "        F.count(\"*\").alias(\"txn_count\"),\n",
    "        F.avg(\"amount\").alias(\"avg_amount\")\n",
    "    )\n",
    "\n",
    "    # Write to a staging table in PostgreSQL\n",
    "    grouped_df.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"staging_customer_merchant_txn_counts\",\n",
    "        mode=\"overwrite\",\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "    # Upsert the data into the main table\n",
    "    upsert_sql = \"\"\"\n",
    "        INSERT INTO customer_merchant_txn_counts (customer, merchant, txn_count, avg_amount)\n",
    "        SELECT customer, merchant, txn_count, avg_amount\n",
    "        FROM staging_customer_merchant_txn_counts\n",
    "        ON CONFLICT (customer, merchant)\n",
    "        DO UPDATE \n",
    "        SET txn_count = customer_merchant_txn_counts.txn_count + EXCLUDED.txn_count,\n",
    "            avg_amount = (\n",
    "                (customer_merchant_txn_counts.avg_amount * customer_merchant_txn_counts.txn_count + \n",
    "                 EXCLUDED.avg_amount * EXCLUDED.txn_count) /\n",
    "                (customer_merchant_txn_counts.txn_count + EXCLUDED.txn_count)\n",
    "            );\n",
    "    \"\"\"\n",
    "    cur.execute(upsert_sql)\n",
    "    conn.commit()\n",
    "\n",
    "    # Drop the staging table\n",
    "    cur.execute(\"DROP TABLE IF EXISTS staging_customer_merchant_txn_counts\")\n",
    "    conn.commit()\n",
    "\n",
    "    # Group by merchant, count gender distribution\n",
    "    grouped_gender = df.groupBy(\"merchant\").agg(\n",
    "        F.sum(F.when(col(\"gender\") == \"'M'\", 1).otherwise(0)).alias(\"male\"),\n",
    "        F.sum(F.when(col(\"gender\") == \"'F'\", 1).otherwise(0)).alias(\"female\")\n",
    "    )\n",
    "\n",
    "    # Write gender counts to staging table\n",
    "    grouped_gender.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"staging_merchant_gender_counts\",\n",
    "        mode=\"overwrite\",\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "    # Upsert gender counts into the main table\n",
    "    upsert_sql1 = \"\"\"\n",
    "        INSERT INTO merchant_gender_counts (merchant, male, female)\n",
    "        SELECT merchant, male, female\n",
    "        FROM staging_merchant_gender_counts\n",
    "        ON CONFLICT (merchant)\n",
    "        DO UPDATE \n",
    "        SET male = merchant_gender_counts.male + EXCLUDED.male,\n",
    "            female = merchant_gender_counts.female + EXCLUDED.female;\n",
    "    \"\"\"\n",
    "    cur.execute(upsert_sql1)\n",
    "    conn.commit()\n",
    "\n",
    "    # Drop the staging gender table\n",
    "    cur.execute(\"DROP TABLE IF EXISTS staging_merchant_gender_counts\")\n",
    "    conn.commit()\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# Pattern 1: Top 1% high-transacting customers per merchant\n",
    "def pat1(jdbc_url, properties):\n",
    "    customer_merchant_txn_counts = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"customer_merchant_txn_counts\",\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "    # Define a window to rank by transaction count\n",
    "    window_spec_by_merchant = Window.partitionBy(\"merchant\").orderBy(F.desc(\"txn_count\"))\n",
    "\n",
    "    ranked_df = customer_merchant_txn_counts.withColumn(\"txn_rank\", F.rank().over(window_spec_by_merchant))\n",
    "    ranked_df = ranked_df.withColumn(\"total_txn\", F.sum(\"txn_count\").over(window_spec_by_merchant))\n",
    "\n",
    "    # Filter top 1% customers\n",
    "    top_1pct_customers_df = ranked_df.filter(\n",
    "        (col(\"txn_rank\") <= F.greatest(F.lit(1), (col(\"total_txn\") * 0.01).cast(\"int\")))\n",
    "    ).select(\"customer\", \"merchant\", \"txn_count\", \"txn_rank\", \"total_txn\")\n",
    "\n",
    "    bottom_1pct_weights_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"bottom_1pct_weights\",\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "    # Join with weight data\n",
    "    joined_df = top_1pct_customers_df.alias(\"t\").join(\n",
    "        bottom_1pct_weights_df.alias(\"b\"),\n",
    "        on=[\"customer\", \"merchant\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Add metadata columns\n",
    "    pattern_matches_df = joined_df.withColumn(\"ystarttime\", F.current_timestamp())\n",
    "    pattern_matches_df = pattern_matches_df.withColumn(\"detectiontime\", F.current_timestamp())\n",
    "    pattern_matches_df = pattern_matches_df.withColumn(\"patternid\", F.lit(\"PatId1\"))\n",
    "    pattern_matches_df = pattern_matches_df.withColumn(\"actiontype\", F.lit(\"UPGRADE\"))\n",
    "\n",
    "    # Select final output columns\n",
    "    pattern_matches_df = pattern_matches_df.select(\n",
    "        \"ystarttime\", \"detectiontime\", \"patternid\", \"actiontype\", \"customer\", \"merchant\"\n",
    "    )\n",
    "\n",
    "    return pattern_matches_df\n",
    "\n",
    "# Pattern 2: Child customers based on avg amount and txn count\n",
    "def pat2(jdbc_url, properties):\n",
    "    customer_merchant_txn_counts = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"customer_merchant_txn_counts\",\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "    # Identify customers with low avg amount but high txn count\n",
    "    child_customers_df = customer_merchant_txn_counts.filter(\n",
    "        (col(\"avg_amount\") < 23) & (col(\"txn_count\") >= 80)\n",
    "    )\n",
    "\n",
    "    # Add metadata\n",
    "    child_customers_df = child_customers_df.withColumn(\"ystarttime\", F.current_timestamp())\n",
    "    child_customers_df = child_customers_df.withColumn(\"detectiontime\", F.current_timestamp())\n",
    "    child_customers_df = child_customers_df.withColumn(\"patternid\", F.lit(\"PatId2\"))\n",
    "    child_customers_df = child_customers_df.withColumn(\"actiontype\", F.lit(\"CHILD\"))\n",
    "\n",
    "    # Select final output columns\n",
    "    child_customers_df = child_customers_df.select(\n",
    "        \"ystarttime\", \"detectiontime\", \"patternid\", \"actiontype\", \"customer\", \"merchant\"\n",
    "    )\n",
    "\n",
    "    return child_customers_df\n",
    "\n",
    "# Pattern 3: Gender imbalance at merchant level\n",
    "def pat3(jdbc_url, properties):\n",
    "    merchant_gender = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"merchant_gender_counts\",\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "    # Identify merchants with gender imbalance\n",
    "    merchant_gender = merchant_gender.filter((col('male') > col('female')) & (col('female') > 100))\n",
    "\n",
    "    # Add metadata\n",
    "    merchant_gender = merchant_gender.withColumn(\"ystarttime\", F.current_timestamp())\n",
    "    merchant_gender = merchant_gender.withColumn(\"detectiontime\", F.current_timestamp())\n",
    "    merchant_gender = merchant_gender.withColumn(\"patternid\", F.lit(\"PatId3\"))\n",
    "    merchant_gender = merchant_gender.withColumn(\"actiontype\", F.lit(\"DEI-NEEDED\"))\n",
    "\n",
    "    # Final selection with customer as empty\n",
    "    merchant_gender = merchant_gender.select(\n",
    "        \"ystarttime\", \"detectiontime\", \"patternid\", \"actiontype\",\n",
    "        F.lit(\"\").alias(\"customer\"),\n",
    "        \"merchant\"\n",
    "    )\n",
    "\n",
    "    return merchant_gender\n",
    "\n",
    "# PostgreSQL connection helper\n",
    "def postgres_cur():\n",
    "    host = \"database-2.czy4sq8iitbm.ap-south-1.rds.amazonaws.com\"\n",
    "    port = \"5432\"\n",
    "    db = \"postgres\"\n",
    "    user_name = \"postgres\"\n",
    "    user_pass = postgres_pwd\n",
    "    conn = psycopg2.connect(host=host, port=port, database=db, user=user_name, password=user_pass)\n",
    "    return conn, conn.cursor()\n",
    "\n",
    "# Get next data chunk to process\n",
    "def get_next_chunk():\n",
    "    conn, cur = postgres_cur()\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT id, chunk_index, s3_path \n",
    "        FROM chunk_tracking \n",
    "        WHERE status = 'written' \n",
    "        ORDER BY chunk_index ASC \n",
    "        LIMIT 1\n",
    "        FOR UPDATE SKIP LOCKED;\n",
    "    \"\"\")\n",
    "    data = cur.fetchone()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return data\n",
    "\n",
    "# De-duplicate entries in final_output based on customer + merchant + ystarttime\n",
    "def dedupe_final_output():\n",
    "    conn, cur = postgres_cur()\n",
    "    cur.execute(\"\"\"\n",
    "        WITH ranked_rows AS (\n",
    "            SELECT ctid,\n",
    "                   ROW_NUMBER() OVER (\n",
    "                       PARTITION BY customer, merchant\n",
    "                       ORDER BY ystarttime ASC\n",
    "                   ) AS rn\n",
    "            FROM final_output\n",
    "        )\n",
    "        DELETE FROM final_output\n",
    "        WHERE ctid IN (\n",
    "            SELECT ctid FROM ranked_rows WHERE rn > 1\n",
    "        );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    # cur.close()\n",
    "    # conn.close()\n",
    "    return True\n",
    "\n",
    "# Update the status of a chunk after processing\n",
    "def update_chunk_status(chunk_index, status):\n",
    "    conn, cur = postgres_cur()\n",
    "\n",
    "    cur.execute(\"BEGIN;\")\n",
    "\n",
    "    # Lock the row first to prevent race conditions\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT * FROM chunk_tracking\n",
    "        WHERE chunk_index = %s\n",
    "        FOR UPDATE;\n",
    "    \"\"\", (chunk_index,))\n",
    "\n",
    "    # Update the status with timestamp\n",
    "    cur.execute(\"\"\"\n",
    "        UPDATE chunk_tracking \n",
    "        SET status = %s, updated_at = %s \n",
    "        WHERE chunk_index = %s;\n",
    "    \"\"\", (status, t.utcnow(), chunk_index))\n",
    "\n",
    "    cur.execute(\"COMMIT;\")\n",
    "    conn.commit()\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# List CSV files in S3 path\n",
    "def list_s3_files(bucket_name, prefix):\n",
    "    s3_client = boto3.client('s3')\n",
    "    file_list = []\n",
    "\n",
    "    if prefix and not prefix.endswith('/'):\n",
    "        prefix += '/'\n",
    "\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    try:\n",
    "        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "            if 'Contents' in page:\n",
    "                for obj in page['Contents']:\n",
    "                    file_list.append(obj['Key'])\n",
    "\n",
    "        # Filter for .csv files\n",
    "        file_list = [file for file in file_list if file.lower().endswith('.csv')]\n",
    "        return file_list[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Generate the final output to be published\n",
    "def generate_output():\n",
    "    conn, cur = postgres_cur()\n",
    "    query = \"\"\"         \n",
    "        WITH to_publish AS (\n",
    "            SELECT merchant, customer\n",
    "            FROM final_output\n",
    "            WHERE published IS NULL\n",
    "            ORDER BY ystarttime\n",
    "            LIMIT 50\n",
    "            FOR UPDATE SKIP LOCKED\n",
    "        ),\n",
    "        check_count AS (\n",
    "            SELECT COUNT(*) AS cnt FROM to_publish\n",
    "        ),\n",
    "        update_rows AS (\n",
    "            UPDATE final_output\n",
    "            SET published = 'published'\n",
    "            WHERE (merchant, customer) IN (\n",
    "                SELECT merchant, customer FROM to_publish\n",
    "            )\n",
    "            AND EXISTS (\n",
    "                SELECT 1 FROM check_count WHERE cnt = 50\n",
    "            )\n",
    "            RETURNING *\n",
    "        )\n",
    "        SELECT * FROM update_rows;\n",
    "    \"\"\"\n",
    "    cur.execute(\"BEGIN;\")\n",
    "    cur.execute(query)\n",
    "    published_rows = cur.fetchall()\n",
    "    cur.execute(\"COMMIT;\")\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"ystarttime\", TimestampType(), True),\n",
    "        StructField(\"detectiontime\", TimestampType(), True),\n",
    "        StructField(\"patternid\", StringType(), True),\n",
    "        StructField(\"actiontype\", StringType(), True),\n",
    "        StructField(\"customer\", StringType(), True),\n",
    "        StructField(\"merchant\", StringType(), True),\n",
    "        StructField(\"published\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # Convert to Spark DataFrame\n",
    "    df = spark.createDataFrame(published_rows, schema=schema)\n",
    "    df = df.select(\n",
    "        col(\"ystarttime\"),\n",
    "        col(\"detectiontime\"),\n",
    "        col(\"patternid\"),\n",
    "        col(\"actiontype\"),\n",
    "        col(\"customer\"),\n",
    "        col(\"merchant\")\n",
    "    )\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e47fa7ec-dae2-4e95-aeb7-238b3280807a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Producer Process which creates chunks of 10K records every nth second to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd923c73-62dc-4d9b-b63d-cce8e6c9a775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_csv_path = \"s3://devdolphins007/Source/transactions.csv\"   # Change to your CSV file path\n",
    "output_s3_prefix = \"s3://devdolphins007/raw/chunk_\"  # Output path prefix\n",
    "chunk_size = 10000\n",
    "sleep_seconds = 3\n",
    "\n",
    "conn,cur = postgres_cur()\n",
    "\n",
    "# Load CSV\n",
    "df = spark.read.csv(input_csv_path, header=True, inferSchema=True)\n",
    "\n",
    "# Total row count\n",
    "total_rows = df.count()\n",
    "total_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "# Loop through chunks\n",
    "for i in range(total_chunks):\n",
    "    offset = i * chunk_size\n",
    "    chunk_df = df.limit(chunk_size + offset).subtract(df.limit(offset))\n",
    "\n",
    "    output_path = f\"{output_s3_prefix}{i}\"\n",
    "    record_count = chunk_df.count()\n",
    "\n",
    "    # Write to S3\n",
    "    chunk_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "    \n",
    "    # Insert into PostgreSQL\n",
    "    try:\n",
    "        cur.execute(\"\"\"\n",
    "                            INSERT INTO chunk_tracking (chunk_index, s3_path, record_count, status, created_at)\n",
    "                            VALUES (%s, %s, %s, %s, %s)\n",
    "                    \"\"\",(i, output_path, record_count, 'written',t.utcnow()))\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"[{i+1}/{total_chunks}] Written to S3 and logged to DB: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"DB insert failed for chunk {i}: {e}\")\n",
    "        conn.rollback()\n",
    "    time.sleep(sleep_seconds)\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7387f4ce-a4e0-4d6e-a7e0-99b47e8eadfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Consumer process which consumer 10k records from S3 in seqeunce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09385ba-ab2e-4950-8479-31ac3146a682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PostgreSQL setup\n",
    "pg_conn,pg_cursor = postgres_cur()\n",
    "# S3 setup\n",
    "s3 = boto3.client('s3')  # assumes credentials are set via env, IAM role, or config\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        \n",
    "        chunk = get_next_chunk()\n",
    "        if not chunk:\n",
    "            print(\"No more chunks to process.\")\n",
    "            break\n",
    "        \n",
    "        chunk_id, chunk_index, s3_path = chunk\n",
    "        print(f\"Picked chunk {chunk_index} from {s3_path}\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            bucket = \"devdolphins007\"\n",
    "            key   =  f\"raw/chunk_{chunk_index}\"\n",
    "\n",
    "            file_name = list_s3_files(bucket,key)\n",
    "            key = file_name\n",
    "\n",
    "\n",
    "            try:\n",
    "                s3.head_object(Bucket=bucket, Key=key)\n",
    "            except Exception as e:\n",
    "                print(f\"S3 head object is failing {chunk_index}:{e}\")                      \n",
    "            \n",
    "            try:\n",
    "                # print(f\"s3://{bucket}/{file_name}\")\n",
    "                df = spark.read.option(\"header\", \"true\").csv(f\"s3://{bucket}/{file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f'Can\\'t load DF {chunk_index}:{e}')\n",
    "\n",
    "            try:\n",
    "                update_postgres_data(df)\n",
    "                pat1_df = pat1(jdbc_url,properties)\n",
    "                pat2_df = pat2(jdbc_url,properties)\n",
    "                pat3_df = pat3(jdbc_url,properties)\n",
    "\n",
    "                union_df = pat1_df.unionByName(pat2_df).unionByName(pat3_df)\n",
    "                # .withColumn('published',F.lit(None))\n",
    "            \n",
    "                union_df.write.jdbc(url=jdbc_url,table=\"final_output\", mode=\"append\",properties=properties)\n",
    "\n",
    "                if dedupe_final_output():\n",
    "                    output_df = generate_output()\n",
    "                    if output_df.count() ==50:\n",
    "                        output_s3_prefix = \"s3://devdolphins007/processed/chunk_\"\n",
    "                        output_path = f\"{output_s3_prefix}{chunk_index}\"\n",
    "                        output_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "                        print(f'output written {chunk_index}')\n",
    "\n",
    "                update_chunk_status(chunk_index, 'processed')\n",
    "                print(f\"Chunk {chunk_index} processed.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'Processing failed {chunk_index}:{e}')\n",
    "                update_chunk_status(chunk_index, 'failed')\n",
    "\n",
    "            # time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {chunk_index}: {e}\")\n",
    "            update_chunk_status(chunk_index, 'failed')\n",
    "\n",
    "finally:\n",
    "    pg_cursor.close()\n",
    "    pg_conn.close()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DevDolphins_Producer_Consumer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}